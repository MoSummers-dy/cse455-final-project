<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="Diana Dai" />
        <title>Petals to the Metal - Flower Classification on TPU</title>
        <link rel="icon" type="image/x-icon" href="assets/rose.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
		<!-- Highlights for code display -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/atom-one-dark.min.css">
    </head>
    <body>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    	<script>hljs.highlightAll();</script>
        <!-- Page Header -->
        <header class="masthead" style="background-image: url('assets/img/flower1.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>Petals to the Metal</h1>
                            <h2 class="subheading">Flower Classification on TPU</h2>
                            <span class="meta">
                                Written by
                                <a href="#!">Diana Dai, Alisa Shi</a>
                                on Mar 14, 2022
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container">
                <!-- Side navigation -->
                <div class="row py-3">
                    <div class="col-9 order-2">
                        <nav class="nav navbar-dark sticky-top flex-column py-3">
                            <ul>
                                <li class="list-group-item list-group-item-action"><a href="#summary" class="nav-link">Summary</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#setup" class="nav-link">Motivation & Setup</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#data" class="nav-link">Dataset</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#tech" class="nav-link">Techniques & Evaluations</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#takeaway" class="nav-link">Challenges & Takeaways</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#video" class="nav-link">Detailed Video</a></li>
                                <li class="list-group-item list-group-item-action"><a href="#ref" class="nav-link">References</a></li>
                            </ul>
                        </nav>
                    </div>
                    <div class="col" id="main">

                        <!-- Summary -->
                        <a name="summary"></a>
                        <h1>I. Summary</h1>
                        <p>
                            we competed in a Kaggle competition<a
                            href="https://www.kaggle.com/c/tpu-getting-started/overview"><sup>[1]</sup></a>
                            to develop a classifier that classifies 104 types of flowers. We started
                            with provided code using a <strong>convolution neural net (CNN)</strong> architecture
                            <code>VGG16</code> to train the model with transfer training, and deployed
                            <strong>seven different significant modifications</strong> to increase the performance of
                            the classifier:
                        </p>
                        <ul>
                            <li><a href="#mod1">Modification 1: Train with other models</a></li>
                            <li><a href="#mod2">Modification 2: Calculate weights</a></li>
                            <li><a href="#mod3">Modification 3: Change optimizers</a></li>
                            <li><a href="#mod4">Modification 4: Mix models</a></li>
                            <li><a href="#mod5">Modification 5: Augment images</a></li>
                            <li><a href="#mod6">Modification 6: Increase number of epochs</a></li>
                            <li><a href="#mod7">Modification 7: Incorporate external data</a></li>
                        </ul>
                        <a name="list"><span class="caption text-muted">Click to go to specific modification</span></a>
                        <p>
                            We compare the models based on their f1-score, precision, and recall on
                            their predictions on the validation set. <strong>Our best model is a combination
                            of Xception and EfficientNet architecture, with extra training data and
                            images augmented with random blocking.</strong> The starter code has a score of
                            0.04083, while <strong>our best model got a score of 0.95901, which places us in
                            23rd on the leaderboard.</strong> We also discuss the challenges and takeaways
                            towards the end of this report.
                        </p>
                        <p>
                            Source code for our best model (see history versions of the notebook shows our various modifications):
                            <ul>
                                <li><a href="https://github.com/MoSummers-dy/cse455-final-project/blob/main/iter7-extra-training.ipynb">
                                    <u>Jupyter Notebook</u></a></li>
                                <li><a
                                href="https://www.kaggle.com/mosummers/petals-to-the-metal-cv-project">
                                    <u>Kaggle Notebook</u></a></li>
                                <li>
                                    <a href="https://docs.google.com/document/d/1W8seR7kPzklhTNjTv2NAMuiUA9yCf88763LlOnq8Dw8/edit?usp=sharing">
                                        <u>Intermediate epoch records and experiments</u></a>
                                </li>
                            </ul>
                        </p>

                        <p>For a video summary of this project, please see&nbsp;below.
                        A detailed video walk-through is also available&nbsp; in the VI. Video section below.&nbsp;</p>
                        <!-- TODO -->
                        <p>[Insert YouTube Video Frame]</p>

                        <!-- Motivation & Setup -->
                        <a name="setup"></a>
                        <h1>II. Motivation & Setup</h1>
                        <p>There are a lot of species in nature, for over 5,000
                        species of mammals, 10,000 species of birds, and 30,000 species of fish. How
                        about some plants? Well, we have over 400,000 different types of flowers!
                        Incredible! For this competition, each team is challenged to build a machine
                        learning model that identifies the type of flowers in a dataset of images.
                        The good news is, we will have just over 100 types, so the work is
                        easier.</p>
                        <p>Our project builds upon&nbsp;Ryan&apos;s notebook<a
                            href="https://www.kaggle.com/ryanholbrook/create-your-first-submission"><sup>[2]</sup></a>
                             on how to build an image classifier in Keras and train it on Tensor
                             Processing Unit (TPU). We then expand from there
                            to train with other models and techniques with ideas of our own.</p>
                        <p>
                            A TPU<a href="https://www.kaggle.com/docs/tpu"><sup>[3]</sup></a>
                            has eight different cores, and each of these cores acts as its own
                        accelerator. A TPU is sort of like having eight GPUs on one machine. We
                        used TPUs for this challenge because these are equipped with 128 GB
                        high-speed memory, so even with 512x512 pixels input images (provided in the
                        dataset as well), TPU can handle these properly.
                        </p>

                        <!-- Dataset -->
                        <a name="data"></a>
                        <h1>III. Dataset</h1>
                        <p>The Kaggle competition provides&nbsp; data<a
                        href="https://www.kaggle.com/c/tpu-getting-started/data"><sup>[4]</sup></a>
                        in&nbsp; TFRecords<a href="https://www.kaggle.com/ryanholbrook/tfrecords-basics"><sup>[5]</sup></a> format, which fits well into TPUs. We have:</p>
                        <ul>
                            <li>train/*.tfrec - training samples, including labels.
                            </li>
                            <li>val/*.tfrec - pre-split training samples w/ labels intended to help with checking your model&apos;s performance on TPU. The split was stratified across labels.
                            </li>
                            <li>test/*.tfrec - samples without labels - we&apos;ll be predicting what classes of flowers these fall into.
                            </li>
                            <li>sample_submission.csv - a sample submission file in the correct format
                            </li>
                        </ul>
                        <p>In one of the modifications, we add extra data to the training dataset to better train our classifier. More on it in the next section.&nbsp;
                        <div class="row justify-content-center">
                            <div class="col-8">
                                <img class="img-fluid" src="assets/img/mod0-starter/flowers.png" alt="flowers" />
                                <span class="caption text-muted">Flowers from the Training Dataset</span>
                            </div>
                        </div>

                        <!-- Techniques & Evaluations  -->
                        <a name="tech"></a>
                        <h1>IV. Techniques & Evaluations</h1>
                        <p>Now we&apos;re ready to create a neural network for classifying images!
                        <strong>We&apos;ll use what&apos;s known as transfer learning.</strong> With transfer
                        learning, we reuse part of a pre-trained model to get a head-start on a new dataset.&nbsp;</p>
                        <p>For the starter code, it uses a model called <code>VGG16</code> (pre-trained on
                        ImageNet<a
                        href="https://image-net.org/"><sup>[6]</sup></a>).
                        However, the accuracy on the validation set is only about 22%, which is far
                        below what we want. Let us try other models to make it better.</p>
                        <p>
                            Before making the final predictions on the test set, it's a good idea to
                            evaluate model&apos;s predictions on the validation set. This can help
                            diagnose problems in training or suggest ways the model could be
                            improved. <strong>We&apos;ll look at two common ways of validation: plotting the
                            confusion matrix and visual validation.</strong>
                            A confusion matrix shows the actual class of an image tabulated against
                            its predicted class. It is one of the best tools for evaluating the
                            performance of a classifier.
                            It can also be helpful to look at some examples from the validation set
                            and see what class the model predicted. This can help reveal patterns in
                            the kinds of images the model has trouble with.
                        </p>

                        <!-- Modification 1 -->
                        <a name="mod1"></a>
                        <h2>üîî Modification 1: Train with Other Models</h2>
                        <p>We want to experiment with other models included with Keras.</p>
                        <p>We start simple. We stick with <code>VGG16</code> for now, but we would like to change
                        the trainable attribute so that it unfreezes the layers and keep changing
                        the values<a
                        href="https://www.tensorflow.org/guide/keras/transfer_learning"><sup>[7]</sup></a>.
                        After training, we do see slight improvements and the accuracy of the
                        validation set goes from 22% to 26%. We decided to move on with
                        Trainable=True for now with other models.</p>
                        <p>We then continue to train with <code>VGG19()</code>, <code>Xception()</code>, <code>DenseNet201()</code>, <code>EfficientNetB7()</code>, <code>InceptionV3()</code>, <code>MobileNetV3Small()</code>, <code>NASNetMobile()</code>, and <code>ResNet152V2()</code>.&nbsp;</p>
                        <div class="row justify-content-center align-items-center">
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/mobileNet1.png" alt="denseNet2" />
                                <span class="caption text-muted"><code>MobileNetV3Small()</code> Confusion Matrix</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/nasNet2.png" alt="nasNet1" />
                                <span class="caption text-muted"><code>NASNetMobile()</code> Confusion Matrix</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/resNet2.png" alt="res2" />
                                <span class="caption text-muted"><code>ResNet152V2()</code> Confusion Matrix</span>
                            </div>
                        </div>
                        <div class="row justify-content-center align-items-center">
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/denseNet3.png" alt="denseNet3" />
                                <span class="caption text-muted"><code>DenseNet201()</code> Visual Validation</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/efficientNet3.png" alt="efn3" />
                                <span class="caption text-muted"><code>EfficientNetB7()</code> Visual Validation</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod1/xception2.png" alt="xception2" />
                                <span class="caption text-muted"><code>Xception()</code> Visual Validation</span>
                            </div>
                        </div>
                        <p>Some of these models perform worse than VGG16, such as VGG 19 (15.17%
                        accuracy on validation set), while others do have notable improvements. Let&apos;s do
                        a quick comparisons for models that perform with an accuracy higher than 85%
                        on validation set:</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Model</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>Xception</td>
                                <td>0.936</td>
                                <td>0.944</td>
                                <td>0.932</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>DenseNet201s</td>
                                <td>0.942</td>
                                <td>0.944</td>
                                <td>0.944</td>
                              </tr>
                              <tr>
                                <th scope="row">3</th>
                                <td>EfficientNetB7 with weights = &apos;imagenet&apos;</td>
                                <td>0.895</td>
                                <td>0.901</td>
                                <td>0.895</td>
                              </tr>
                            </tbody>
                          </table>
                        <p>One thing to note is that when we were experimenting with our ‚öñÔ∏è
                        Modification 2, we came across a blog saying how the performance can be
                        improved if we changed the value assigned to the weights attribute.
                        Unfortunately, the Keras library does not support &apos;noisy-student&apos;
                        weights. So we need to import efficientNet and go from
                        there. The accuracy improved a lot (from 90% to 95%) by changing the weights
                        from &lsquo;imagenet&rsquo; to &lsquo;noisy-student&rsquo;! It seems like
                        noisy-student is a better weight to be used for EfficientNetB7. We will
                        use it from now on.</p>

                        <!-- Modification 2 -->
                        <a name="mod2"></a>
                        <h2>‚öñÔ∏è Modification 2: Calculate Weights</h2>
                        <p>For this modification, the idea is to adjust the weights to
                        &quot;pay more attention&quot; to the under-represented classes that only
                        have a small amount of available samples.</p>
                        <p>Inside Keras&apos;s <code>model.fit()</code>, there is an attribute called <code>class_weight</code><a
                            href="https://keras.io/api/models/sequential/"><sup>[8]</sup></a>. This
                            attribute is an &quot;optional dictionary mapping class indices (integers)
                            to a weight (float) value, used for weighting the loss function (during
                            training only)&quot;. It is set up for&nbsp;classification
                            on imbalanced data<a
                            href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"><sup>[9]</sup></a>.</p>
                        <p>Maybe we can customize this attribute? Let us start by calculating the
                        percentage of each class presented in the training dataset. Here is a graph
                        that show the different weights &ndash; an inverse of percentage:</p>
                        <img class="img-fluid" src="assets/img/mod2/classWeights.png" alt="class weights" />
                        <span class="caption text-muted">Class Weights (inverse of %)</span>
                        <p>We pick the models that perform pretty well in the last round: <code>Xception()</code>, <code>DenseNet201()</code>, <code>EfficientNetB7()</code>, <code>InceptionV2()</code>, and <code>ResNet152V2()</code>, and train these with class weights.</p>
						<pre><code class="language-python">
history = model.fit(
    ds_train,
    validation_data=ds_valid,
    epochs=EPOCHS,
    steps_per_epoch=STEPS_PER_EPOCH,
    callbacks=[lr_callback],
    # modification 2: class weights (abandoned because no improvements)
    class_weight = training_class_weights
    )
						</code></pre>
                        <p>Another quick comparison on f1, precision, and recall (before -&gt; after
                        applying class weights):</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Model</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>Xception</td>
                                <td>0.936 -&gt; 0.941</td>
                                <td>0.944 -&gt; 0.940</td>
                                <td>0.932 -&gt; 0.946</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>DenseNet201s</td>
                                <td>0.942 -&gt; 0.935</td>
                                <td>0.944 -&gt; 0.931</td>
                                <td>0.944 -&gt; 0.944</td>
                              </tr>
                              <tr>
                                <th scope="row">3</th>
                                <td>EfficientNetB7 with weights = &apos;imagenet&apos;</td>
                                <td>0.895 -&gt; 0.901</td>
                                <td>0.901 -&gt; 0.894</td>
                                <td>0.895 -&gt; 0.914</td>
                              </tr>
                              <tr>
                                <th scope="row">4</th>
                                <td>EfficientNetB7 with weights = &apos;noisy-student&apos;</td>
                                <td>0.949 -&gt; 0.941</td>
                                <td>0.948 -&gt; 0.932</td>
                                <td>0.955 -&gt; 0.955</td>
                              </tr>
                              <tr>
                                <th scope="row">5</th>
                                <td>InceptionV3</td>
                                <td>0.925 -&gt; 0.907</td>
                                <td>0.930 -&gt; 0.899</td>
                                <td>0.926 -&gt; 0.922</td>
                              </tr>
                              <tr>
                                <th scope="row">6</th>
                                <td>InceptionV2</td>
                                <td>0.898 -&gt; 0.812</td>
                                <td>0.912 -&gt; 0.801</td>
                                <td>0.891 -&gt; 0.893</td>
                              </tr>
                            </tbody>
                          </table>
                        <p>From the results, <strong>we do not see significant improvement with
                        customized weights.</strong> Therefore, we are not going to use
                        <code>training_class_weights</code> in future iterations.</p>
                        <!-- TODO: <code> -->
                        <p>We also decide to remove <code>EfficientNetB7()</code> with weights =
                        &apos;imagenet&apos;, <code>InceptionV3()</code> and <code>RasNet152V2()</code> from the list of
                        usable models because their accuracy is the relatively low among all in both
                        rounds now.</p>

                        <!-- Modification 3 -->
                        <a name="mod3"></a>
                        <h2>üöÄ Modification 3: Change Optimizers</h2>
                        <p>How about optimizers? What are some optimizers that are available and
                        maybe have a better performance? Since we do have time limitation on running
                        with TPUs, we cannot train on every model that we found in the first
                        modifications. We will start with <code>Xception()</code> for now because of its faster
                        runtime among all models with relatively high accuracy.</p>
                        <p>Before we get started exploring, there is some notes on choosing the
                        optimizers. Even though we have decided to not continue with training with
                        class weights calculated from training dataset, there is still some facts we
                        found out about our data. Namely, our dataset is imbalanced. So it is
                        possible that some optimizers will perform worse than <code>Adam</code> (in the starter
                        code), which is okay because we can always go back to it. But let&apos;s
                        start experimenting first:</p>
                        <pre><code class="language-python">
model.compile(
    optimizer='adam',
#     optimizer = 'Adadelta',
#     optimizer = 'Adagrad',
#     optimizer = 'Adamax',
#     optimizer = 'Ftrl',
#     optimizer = 'Nadam',
#     optimizer = 'RMSprop',
#     optimizer = 'SGD',
    loss = 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy'],
)
                        </code></pre>
                        <div class="row justify-content-center align-items-center">
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod3/adamax.png" alt="adamax" />
                                <span class="caption text-muted">Adamax Confusion Matrix</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod3/nadam.png" alt="nadam" />
                                <span class="caption text-muted">Nadam Confusion Matrix</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod3/rms.png" alt="rms" />
                                <span class="caption text-muted">RMSprop Confusion Matrix</span>
                            </div>
                        </div>
                        <p>Again, let&apos;s do a comparison of the optimizers with relatively high accuracy on validation set:</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Model</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>Adam</td>
                                <td>0.936</td>
                                <td>0.944</td>
                                <td>0.932</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>Adamax</td>
                                <td>0.921</td>
                                <td>0.933</td>
                                <td>0.918</td>
                              </tr>
                              <tr>
                                <th scope="row">3</th>
                                <td>Nadam</td>
                                <td>0.937</td>
                                <td>0.942</td>
                                <td>0.936</td>
                              </tr>
                              <tr>
                                <th scope="row">4</th>
                                <td>RMSprop</td>
                                <td>0.937</td>
                                <td>0.941</td>
                                <td>0.937</td>
                              </tr>
                            </tbody>
                          </table>
                        <p><strong>The improvements by changing the optimizers is not significant
                        either.</strong> Since we cannot predict how the optimizers will perform on
                        <code>DenseNet201()</code> and <code>EfficientNetB7()</code>, we will stick with Adam, which we know
                        for sure that have a good performance on the other two models.</p>

                        <!-- Modification 4 -->
                        <a name="mod4"></a>
                        <h2>üé® Modification 4: Mix Models</h2>
                        <p>We now want to try mixing and matching models together. We have reduced our
                        models to 3 now (<code>Xception()</code>, <code>DenseNet201()</code>, and
                        <code>EfficientNetB7()</code>), so there will be 2! = 3 combinations of
                        models. We examine each pair, and compare their performance.</p>
                        <p>We need to build and train each model separately first, which is the same
                        code we have been using for modification 1. Next, we need to find the
                        <code>best_alpha</code> to set the weight between the two models<a
                        href="https://www.kaggle.com/wrrosa/tpu-enet-b7-densenet#Finding-best-alpha"><sup>[10]</sup></a>
                        </p>
                        <div class="row justify-content-center align-items-center">
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod4/xcepDense2.png" alt="Xception() + DenseNet201() Combo" />
                                <span class="caption text-muted">Xception() + DenseNet201() Visual Validation</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod4/xcepEfn2.png" alt="Xception() + EfficientNetB7() combo" />
                                <span class="caption text-muted">Xception() + EfficientNetB7() Visual Validation</span>
                            </div>
                            <div class="col-4">
                                <img class="img-fluid" src="assets/img/mod4/denseEfn2.png" alt="dense efn" />
                                <span class="caption text-muted">DenseNet201() + EfficientNetB7() Visual Validation</span>
                            </div>
                        </div>
                        <p>Let&apos;s do another round of comparisons on these three combinations:</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Model</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>Xception() + DenseNet201()</td>
                                <td>0.95</td>
                                <td>0.952</td>
                                <td>0.952</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>Xception() + EfficientNetB7()</td>
                                <td>0.961</td>
                                <td>0.962</td>
                                <td>0.962</td>
                              </tr>
                              <tr>
                                <th scope="row">3</th>
                                <td>DenseNet201() + EfficientNetB7()</td>
                                <td>0.956</td>
                                <td>0.954</td>
                                <td>0.960</td>
                              </tr>
                            </tbody>
                          </table>
                        <p>Seems like the <strong>the combination of <code>Xception()</code> and
                            <code>EfficientNetB7()</code> gives us the best result we have so far in
                            the competition &mdash; a score of 0.95303!</strong></p>
                        <p>For reference, the score by submitting <code>DenseNet201()</code> +
                            <code>EfficientNetB7()</code> is 0.94967, which is also pretty good, but
                            not as significant compared to the combination above:
                            <code>Xception()</code> + <code>EfficientNetB7()</code>.
                             We will stick with this combo for now.</p>
                        <img class="img-fluid" src="assets/img/mod4/mod4-score.png" alt="scores after mod4" />
                        <span class="caption text-muted">Submission Scores after Modification 4</span>
                        <br>

                        <!-- Modification 5 -->
                        <a name="mod5"></a>
                        <h2>üöß Modification 5: Augment Images</h2>
                        <p>In the provided starter code, only one simple data augmentation is done,
                        which is to flip the image horizontally. We encounter two interesting image
                        augmentations, both of which involve some randomness. Let&apos;s test it out.</p>
                        <h3>5.1 Random Blockout</h3>
                        <p>The first image augmentation is adapted from Dmitry&apos;s
                            notebook<a
                        href="https://www.kaggle.com/dmitrynokhrin/densenet201-aug-additional-data"><sup>[11]</sup></a>.
                        The idea is to erase or block out a portion of the training images:</p>
                        <div class="row justify-content-center">
                            <div class="col-8">
                                <img class="img-fluid" src="assets/img/mod5/block1.png" alt="random blocking" />
                                <span class="caption text-muted">Random Blocking</span>
                            </div>
                        </div>
                        <h3>5.2 Random Modifications to Images</h3>
                        <p>The second augmentation is adapted
                            from Xuanzhi Huang and Rahul Paul&apos;s notebook<a
                            href="https://www.kaggle.com/xuanzhihuang/flower-classification-densenet-201"><sup>[12]</sup></a>.
                            It randomly make changes to the images, such as resize,
                        crop, reset brightness, reset saturation, reset contract, blur, etc.:</p>
                        <div class="row justify-content-center">
                            <div class="col-8">
                                <img class="img-fluid" src="assets/img/mod5/change1.png" alt="random modifying" />
                                <span class="caption text-muted">Random Modifications</span>
                            </div>
                        </div>
                        <h3>5.3 Evaluations</h3>
                        <p>Here are the statistics on the validation set:</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Image Augmentation</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>random blocking</td>
                                <td>0.960</td>
                                <td>0.961</td>
                                <td>0.963</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>random changing</td>
                                <td>0.791</td>
                                <td>0.844</td>
                                <td>0.781</td>
                              </tr>
                            </tbody>
                          </table>
                        <p>It seems like <strong>randomly changing the pictures decreases the
                        accuracy significantly, but blocking some parts of the training images does
                        help improve the accuracy by a little.</strong> We will keep that since it does give
                        us better results when submitted to the competition -- 0.95462!</p>
                        <a href="#!"><img class="img-fluid" src="assets/img/mod5/ver12-score.png" alt="score after random blocking" /></a>
                        <span class="caption text-muted">Submission Score after Random Blocking</span>
                        <br>

                        <!-- Modification 6 -->
                        <a name="mod6"></a>
                        <h2>üìà Modification 6: Increase Number of Epochs</h2>
                        <p>In the starter code, the number of epochs is set to 12. We stick with 12
                        for most iterations so far because if we try more epochs, the time needed to
                        train will increase.</p>
                        <p>Since Kaggle has a limitation on 20 hours of TPUs per week, we do not want to
                        spend too much time on &quot;picking the models&quot; round. Now we are
                        kinda happy with what we get so far, let&apos;s try to increase the number
                        of epochs to 20.</p>
                        <div class="row justify-content-center">
                            <div class="col-6">
                                <img class="img-fluid" src="assets/img/mod6/epochs20.png" alt="evaluations after increasing epochs" />
                                <span class="caption text-muted">Evaluations after Increasing Epochs to 20</span>
                            </div>
                        </div>
                        <p>Let&apos;s do a quick comparisons on the validation set evaluations (same
                        combo are used <code>Xception()</code> + <code>EfficientNetB7()</code>
                    ):</p>
                        <table class="table">
                            <thead>
                              <tr>
                                <th scope="col">#</th>
                                <th scope="col">Num of Epochs</th>
                                <th scope="col">F1</th>
                                <th scope="col">Precision</th>
                                <th scope="col">Recall</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th scope="row">1</th>
                                <td>12 epochs</td>
                                <td>0.961</td>
                                <td>0.962</td>
                                <td>0.962</td>
                              </tr>
                              <tr>
                                <th scope="row">2</th>
                                <td>20 epochs</td>
                                <td>0.959</td>
                                <td>0.960</td>
                                <td>0.961</td>
                              </tr>
                            </tbody>
                          </table>

                        <p><strong>The score submitted to the competition does not change much either; it
                        even goes down a bit from 0.95462 to 0.95417</strong>. Hence, we will switch back to
                        12 epochs.</p>

                        <!-- Modification 7 -->
                        <a name="mod7"></a>
                        <h2>üíΩ Modification 7: Incorporate External Data</h2>
                        <p>One of the top-rated notebooks<a
                        href="https://www.kaggle.com/atamazian/fc-ensemble-external-data-effnet-densenet"><sup>[13]</sup></a>
                        mentions how using external data can increase the model&rsquo;s accuracy.
                        This makes a lot of sense theoretically since with more training data, we
                        can refine the classifier. So let&rsquo;s add in more data<a
                        href="https://www.kaggle.com/kirillblinov/tf-flower-photo-tfrec"><sup>[14]</sup></a>!</p>
                        <p>Our updated dataset with the extra training data:
                            <ul>
                                <li>12753 -> 68094 training images</li>
                                <li>3712 validation images</li>
                                <li>7382 unlabeled test images</li>
                            </ul>
                        <p>That is a lot of new data added! We go from 12k to 68k now. The time for
                        training went up, but evaluations and the scores showed its worth:</p>
                        <div class="row justify-content-center align-items-center">
                            <div class="col-6">
                                <img class="img-fluid" src="assets/img/mod7/extraTraining.png" alt="evaluations after adding training data" />
                                <span class="caption text-muted">Evaluations after Adding Training Data</span>
                            </div>
                            <div class="col-6">
                                <img class="img-fluid" src="assets/img/mod7/extraTraining2.png" alt="visual validation after adding training data" />
                                <span class="caption text-muted">Visual Validation after Adding Training Data</span>
                            </div>
                        </div>
                        <br>

                        <!-- Potential Modification 8 -->
                        <h2>ü•á (Potential) Modification 8: Find Best Optimizer for Each Model</h2>
                        <p>In üöÄ Modification 3, we experiment with different optimizers on
                        <code>Xception()</code> model and use the result as a general rule for other
                        models as well. This may not be true, though. If time allows, we will also
                        figure out the best optimizer for each of <code>Xception()</code>,
                        <code>DenseNet201()</code> and <code>EfficientNetB7()</code> architectures.
                        Unfortunately, we are running out of our TPU quotas, so we cannot test it
                        out at this point.</p>

                        <!-- Submissions -->
                        <h2>Submissions&nbsp;</h2>
                        <p>In the end, after exhausting all the TPU quotas we have, we made 7 submissions to the competitions:</p>
                        <ul>
                            <li>Version 1: Starter code, score 0.04083</li>
                            <li>Version 3: <code>DenseNet201()</code>, score 0.94145</li>
                            <li>Version 9: Mix <code>DenseNet201()</code> + <code>EfficientNetB7(&apos;noisy-student&apos;)</code>, score 0.94967</li>
                            <li>Version 11: Mix <code>Xception()</code> + <code>EfficientNetB7(&apos;noisy-student&apos;)</code>, score 0.95303</li>
                            <li>Version 12: Version 11 with random blocking, score 0.95462</li>
                            <li>Version 15: Version 12 with 20 epochs, score 0.95417 </li>
                            <li>Version 18: Version 12 with extra training data, score 0.95901&nbsp;</li>
                        </ul>
                        <img class="img-fluid" src="assets/img/mod7/ver18-score-rank23.png" alt="best score" />
                        <span class="caption text-muted">Best Score, Rank #23</span>
                        <br>

                        <!-- Challenges & Takeaways -->
                        <a name="takeaway"></a>
                        <h1>V. Challenges & Takeaways</h1>
                        <p>Overall, it is a fun time to work on the project, and the sense of
                        accomplishment is huge when we see our place goes from 140 to 23 on the
                        leaderboard. But there are some challenges presented as well.
                        <strong>The biggest challenge is that the TPU usage has a time limitation, so we need to give up
                        some potential improvements to train the models within the time frame</strong>, and
                        have to turn off the TPU when we are writing code instead of running
                        it.&nbsp;</p>
                        <div class="row justify-content-center">
                            <div class="col-6">
                                <img class="img-fluid" src="assets/img/mod7/tpuQuota.png" alt="TPU quota" />
                                <span class="caption text-muted">TPU Quota</span>
                            </div>
                        </div>
                        <p><strong>Another challenge is picking the suitable model to
                        train.</strong> In the Keras library,
                        there are a lot of models that we did not get to explore in modification 1.
                        We only get to choose 10 architectures, so the strategy we used is to search
                        for papers that do some comparisons. As an example, for <code>ResNet</code>, a powerful
                        deep neural network idea, there are 8 different models provided. According
                        to a paper<a href="https://arxiv.org/pdf/1512.03385.pdf"><sup>[15]</sup></a> on ResNet,
                        seems like <code>ResNet-152</code> has the least top-1 and top-5 error rates compared to
                        other shallower <code>ResNet</code> models (such as <code>ResNet50</code> or <code>ResNet101</code>) for
                        classification on Imagenet validation. Hence, we use <code>ResNet152V2()</code> instead
                        of others.</p>
                        <p>Throughout this project, we learned about a lot of commonly used
                        architectures, optimizers, and data augmentation strategies. We also gained
                        a lot more experience working with images and transfer training neural
                        networks. It was a lot of fun to read and discuss ideas with other teams on
                        Kaggle. This is the first time we did a competition on Kaggle, and it was
                        definitely worth the time and effort we put in.</p>
                        <p>Future work on this project may include exploring other network
                        architectures, trying out various optimizers for each architecture, and
                        exploring other data augmentation ideas that do not involve so much
                        randomness. We had a great time working on this project and are excited to
                        explore more in the field of computer vision!&nbsp;</p>

                        <!-- Video -->
                        <a name="video"></a>
                        <h1>VI. Detailed Video</h1>
                        <br>
                        <iframe width="722" height="361" src="https://www.youtube.com/embed/4WblQduKdqE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        <br>
                        <br>

                        <!-- References -->
                        <a name="ref"></a>
                        <h1>VII. References</h1>
                        <ul>
                            <li>[1] https://www.kaggle.com/c/tpu-getting-started/overview</li>
                            <li>[2] https://www.kaggle.com/ryanholbrook/create-your-first-submission</li>
                            <li>[3] https://www.kaggle.com/docs/tpu</li>
                            <li>[4] https://www.kaggle.com/c/tpu-getting-started/data</li>
                            <li>[5] https://www.kaggle.com/ryanholbrook/tfrecords-basics</li>
                            <li>[6] https://image-net.org/</li>
                            <li>[7] https://www.tensorflow.org/guide/keras/transfer_learning</li>
                            <li>[8] https://keras.io/api/models/sequential/</li>
                            <li>[9] https://www.tensorflow.org/tutorials/structured_data/imbalanced_data</li>
                            <li>[10] https://www.kaggle.com/wrrosa/tpu-enet-b7-densenet#Finding-best-alpha</li>
                            <li>[11] https://www.kaggle.com/dmitrynokhrin/densenet201-aug-additional-data</li>
                            <li>[12] https://www.kaggle.com/xuanzhihuang/flower-classification-densenet-201</li>
                            <li>[13] https://www.kaggle.com/atamazian/fc-ensemble-external-data-effnet-densenet</li>
                            <li>[14] https://www.kaggle.com/kirillblinov/tf-flower-photo-tfrec</li>
                            <li>[15] https://arxiv.org/pdf/1512.03385.1W8seR7kPzklhTNjTv2NAMuiUA9yCf88763LlOnq8Dw8</li>
                        </ul>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="https://github.com/MoSummers-dy/cse455-final-project">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; CSE 455 Final Project</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
